{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Project\n",
    "This notebook focuses on sentiment analysis in the hospitality domain. The goal is to preprocess text data, extract features, and build models to classify sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "We start by importing the necessary libraries for data manipulation, text preprocessing, and machine learning. Each library serves a specific purpose, such as handling data (pandas), numerical operations (numpy), and text processing (nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zeal.v/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/zeal.v/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/zeal.v/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/zeal.v/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np  # for numerical operations\n",
    "import pandas as pd  # for data manipulation\n",
    "import random  # for shuffling the data\n",
    "import nltk\n",
    "import re  # for handling regular expressions\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  # for lemmatizing words\n",
    "from nltk.corpus import stopwords  # for stop word removal\n",
    "from nltk.tokenize import word_tokenize  # for tokenizing sentences into words\n",
    "\n",
    "# Downloading necessary NLTK resources\n",
    "nltk.download('stopwords')  # List of common stop words in English\n",
    "nltk.download('punkt')  # Pre-trained tokenizer models\n",
    "nltk.download('wordnet')  # WordNet lemmatizer dataset\n",
    "nltk.download('punkt_tab')  # Downloads the 'punkt' tokenizer table used for tokenization of text into sentences or words\n",
    "\n",
    "# Libraries for text feature extraction and model training\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text into numerical features (TF-IDF)\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression for classification\n",
    "from sklearn.svm import LinearSVC  # Support Vector Machines for classification\n",
    "\n",
    "# Libraries for model evaluation\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix  # For model evaluation metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score  # For cross-validation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Libraries\n",
    "Here, we import libraries for visualization (matplotlib, seaborn) and environment variable management (dotenv). These libraries help in creating plots and securely managing file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "We use the `dotenv` library to load environment variables and securely access the file path for the dataset. The dataset is then loaded into a pandas DataFrame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                           Wow... Loved this place.      1\n",
      "1                                 Crust is not good.      0\n",
      "2          Not tasty and the texture was just nasty.      0\n",
      "3  Stopped by during the late May bank holiday of...      1\n",
      "4  The selection on the menu was great and so wer...      1\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get the file path from the .env file\n",
    "file_path = os.getenv('FILE_PATH')\n",
    "\n",
    "# Load the .tsv file\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "The `info()` method provides a summary of the dataset, including column names, data types, and non-null counts. This helps in understanding the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  1000 non-null   object\n",
      " 1   Liked   1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "Text preprocessing is a crucial step in NLP. It involves cleaning and transforming text data into a format suitable for analysis. The steps include converting text to lowercase, tokenization, removing stopwords, and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preprocessing function\n",
    "def preprocess_text(doc):\n",
    "    # Convert to lowercase\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Preprocessing\n",
    "The preprocessing function is applied to the 'Review' column to create a new column 'text_processed'. This column contains the cleaned and tokenized text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0                           Wow... Loved this place.   \n",
      "1                                 Crust is not good.   \n",
      "2          Not tasty and the texture was just nasty.   \n",
      "3  Stopped by during the late May bank holiday of...   \n",
      "4  The selection on the menu was great and so wer...   \n",
      "\n",
      "                                      text_processed  \n",
      "0                                  wow  loved place   \n",
      "1                                        crust good   \n",
      "2                               tasty texture nasty   \n",
      "3  stopped late may bank holiday rick steve recom...  \n",
      "4                       selection menu great prices   \n"
     ]
    }
   ],
   "source": [
    "import string # Import the string module for handling punctuation\n",
    "\n",
    "# Apply the preprocessing function to the 'Review' column\n",
    "df['text_processed'] = df['Review'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(df[['Review', 'text_processed']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             Wow... Loved this place.\n",
       "1                                   Crust is not good.\n",
       "2            Not tasty and the texture was just nasty.\n",
       "3    Stopped by during the late May bank holiday of...\n",
       "4    The selection on the menu was great and so wer...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    wow  loved place \n",
       "1                                          crust good \n",
       "2                                 tasty texture nasty \n",
       "3    stopped late may bank holiday rick steve recom...\n",
       "4                         selection menu great prices \n",
       "Name: text_processed, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "This project aims to analyze customer reviews in the restaurant industry to understand sentiment and improve service quality. By leveraging machine learning, we will classify reviews as positive or negative and extract actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Details\n",
    "The dataset contains two columns:\n",
    "- **Review**: The text of the customer review.\n",
    "- **Liked**: A binary label indicating sentiment (1 for positive, 0 for negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "We will explore the dataset to identify patterns and insights, such as:\n",
    "- Review length and its correlation with sentiment.\n",
    "- Frequent keywords in positive and negative reviews.\n",
    "- Mixed sentiments in reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze review lengths\n",
    "df['review_length'] = df['Review'].apply(len)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='review_length', hue='Liked', bins=30, kde=True)\n",
    "plt.title('Review Length Distribution by Sentiment')\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract frequent keywords\n",
    "from collections import Counter\n",
    "positive_reviews = df[df['Liked'] == 1]['text_processed']\n",
    "negative_reviews = df[df['Liked'] == 0]['text_processed']\n",
    "\n",
    "positive_words = Counter(' '.join(positive_reviews).split())\n",
    "negative_words = Counter(' '.join(negative_reviews).split())\n",
    "\n",
    "print('Most common words in positive reviews:', positive_words.most_common(10))\n",
    "print('Most common words in negative reviews:', negative_words.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "1. Preprocess the text data further if needed.\n",
    "2. Engineer features using TF-IDF or word embeddings.\n",
    "3. Train and evaluate machine learning models.\n",
    "4. Interpret results and provide actionable insights for the restaurant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
