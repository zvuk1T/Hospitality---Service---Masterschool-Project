{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Project\n",
    "This notebook focuses on sentiment analysis in the hospitality domain. The goal is to preprocess text data, extract features, and build models to classify sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "We start by importing the necessary libraries for data manipulation, text preprocessing, and machine learning. Each library serves a specific purpose, such as handling data (pandas), numerical operations (numpy), and text processing (nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zeal.v/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/zeal.v/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/zeal.v/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/zeal.v/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np  # for numerical operations\n",
    "import pandas as pd  # for data manipulation\n",
    "import random  # for shuffling the data\n",
    "import nltk\n",
    "import re  # for handling regular expressions\n",
    "import string  # for string operations\n",
    "from wordcloud import WordCloud  # for generating word clouds\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  # for lemmatizing words\n",
    "from nltk.corpus import stopwords  # for stop word removal\n",
    "from nltk.tokenize import word_tokenize  # for tokenizing sentences into words\n",
    "\n",
    "# Downloading necessary NLTK resources\n",
    "nltk.download('stopwords')  # List of common stop words in English\n",
    "nltk.download('punkt')  # Pre-trained tokenizer models\n",
    "nltk.download('wordnet')  # WordNet lemmatizer dataset\n",
    "nltk.download('punkt_tab')  # Downloads the 'punkt' tokenizer table used for tokenization of text into sentences or words\n",
    "\n",
    "# Libraries for text feature extraction and model training\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text into numerical features (TF-IDF)\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression for classification\n",
    "from sklearn.svm import LinearSVC  # Support Vector Machines for classification\n",
    "\n",
    "# Libraries for model evaluation\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix  # For model evaluation metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score  # For cross-validation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Libraries\n",
    "Here, we import libraries for visualization (matplotlib, seaborn) and environment variable management (dotenv). These libraries help in creating plots and securely managing file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "We use the `dotenv` library to load environment variables and securely access the file path for the dataset. The dataset is then loaded into a pandas DataFrame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                           Wow... Loved this place.      1\n",
      "1                                 Crust is not good.      0\n",
      "2          Not tasty and the texture was just nasty.      0\n",
      "3  Stopped by during the late May bank holiday of...      1\n",
      "4  The selection on the menu was great and so wer...      1\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get the file path from the .env file\n",
    "file_path = os.getenv('FILE_PATH_ZVUK')\n",
    "\n",
    "# Load the .tsv file\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
