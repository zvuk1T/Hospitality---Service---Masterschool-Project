{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee62fcb0-fbfd-4bd5-af37-8547f56f95c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee62fcb0-fbfd-4bd5-af37-8547f56f95c3",
        "outputId": "bb12203f-679b-43ce-a2a2-72ae7b6c2abc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np  # for numerical operations\n",
        "import pandas as pd  # for data manipulation\n",
        "import random  # for shuffling the data\n",
        "import nltk\n",
        "import re  # for handling regular expressions\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer  # for lemmatizing words\n",
        "from nltk.corpus import stopwords  # for stop word removal\n",
        "from nltk.tokenize import word_tokenize  # for tokenizing sentences into words\n",
        "\n",
        "# Downloading necessary NLTK resources\n",
        "nltk.download('stopwords')  # List of common stop words in English\n",
        "nltk.download('punkt')  # Pre-trained tokenizer models\n",
        "nltk.download('wordnet')  # WordNet lemmatizer dataset\n",
        "nltk.download('punkt_tab')  # Downloads the 'punkt' tokenizer table used for tokenization of text into sentences or words\n",
        "\n",
        "# Libraries for text feature extraction and model training\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text into numerical features (TF-IDF)\n",
        "from sklearn.linear_model import LogisticRegression  # Logistic regression for classification\n",
        "from sklearn.svm import LinearSVC  # Support Vector Machines for classification\n",
        "\n",
        "# Libraries for model evaluation\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix  # For model evaluation metrics\n",
        "from sklearn.model_selection import KFold, cross_val_score  # For cross-validation\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5637c05d-10a4-4551-88d8-8de53d7db8a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5637c05d-10a4-4551-88d8-8de53d7db8a1",
        "outputId": "d1367df8-eb22-47f3-93d1-09a5e9f90d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   0\n",
            "0                  simplistic , silly and tedious . \n",
            "1  it's so laddish and juvenile , only teenage bo...\n",
            "2  exploitative and largely devoid of the depth o...\n",
            "3  [garbus] discards the potential for pathologic...\n",
            "4  a visually flashy but narratively opaque and e...\n"
          ]
        }
      ],
      "source": [
        "# Read the positive and negative sentiment files\n",
        "df_sent_pos = pd.read_csv('https://raw.githubusercontent.com/chrisvdweth/nus-cs4248x/refs/heads/master/1-foundations/data/corpora/sentence-polarity-dataset/sentence-polarity.neg', sep='\\t', header=None)  # Positive sentiment sentences\n",
        "df_sent_neg = pd.read_csv('https://raw.githubusercontent.com/chrisvdweth/nus-cs4248x/refs/heads/master/1-foundations/data/corpora/sentence-polarity-dataset/sentence-polarity.pos', sep='\\t', header=None)  # Negative sentiment sentences\n",
        "\n",
        "# Display the first few rows of the positive dataset to understand its structure\n",
        "print(df_sent_pos.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e929ce68-fe35-48ce-80c9-43e764ec4ae7",
      "metadata": {
        "id": "e929ce68-fe35-48ce-80c9-43e764ec4ae7"
      },
      "outputs": [],
      "source": [
        "# Rename the column to 'sentence'\n",
        "df_sent_pos.rename(columns={0: \"sentence\"}, inplace=True)\n",
        "df_sent_neg.rename(columns={0: \"sentence\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3080d66d-2e4e-4a85-a093-911e832dee1c",
      "metadata": {
        "id": "3080d66d-2e4e-4a85-a093-911e832dee1c"
      },
      "outputs": [],
      "source": [
        "# Define the preprocessing function\n",
        "def preprocess_text(sentences):\n",
        "    # Convert all tokens to lowercase\n",
        "    sentences = [sentence.lower() for sentence in sentences]\n",
        "\n",
        "    # Remove punctuation using regex\n",
        "    sentences = [re.sub(r\"[^\\w\\s]\", \"\", sentence) for sentence in sentences]\n",
        "\n",
        "    # Remove extra whitespaces between words\n",
        "    sentences = [\" \".join(sentence.split()) for sentence in sentences]\n",
        "\n",
        "    # Tokenize sentences into words\n",
        "    sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stop words\n",
        "    filtered_sentences = []\n",
        "    for sentence in sentences:\n",
        "        filtered_sentence = [word for word in sentence if word not in stop_words]\n",
        "        filtered_sentences.append(filtered_sentence)\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentences = []\n",
        "    for sentence in filtered_sentences:\n",
        "        lemmatized_sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
        "        lemmatized_sentences.append(lemmatized_sentence)\n",
        "\n",
        "    return [' '.join(sentence) for sentence in lemmatized_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ae052d-b9cd-490b-a05b-d8e22bec6326",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86ae052d-b9cd-490b-a05b-d8e22bec6326",
        "outputId": "893c617d-0621-4637-a6c6-3457256fa1e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rock destined 21st century new conan he going make splash even greater arnold schwarzenegger jeanclaud van damme steven segal\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the sentences\n",
        "pos_preprocessed_sentences = preprocess_text(df_sent_pos['sentence'])\n",
        "neg_preprocessed_sentences = preprocess_text(df_sent_neg['sentence'])\n",
        "\n",
        "# Print the first preprocessed negative sentence\n",
        "print(neg_preprocessed_sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aba38a9b-eef8-41a0-ad06-e15961bcbb28",
      "metadata": {
        "id": "aba38a9b-eef8-41a0-ad06-e15961bcbb28"
      },
      "outputs": [],
      "source": [
        "# Combine preprocessed positive and negative sentences\n",
        "sentences = pos_preprocessed_sentences + neg_preprocessed_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2c32ba9-a00b-4021-95ad-565f00bd685f",
      "metadata": {
        "id": "e2c32ba9-a00b-4021-95ad-565f00bd685f"
      },
      "outputs": [],
      "source": [
        "# Create a list for all labels\n",
        "polarities = []\n",
        "polarities.extend([0] * len(df_sent_neg))  # Label negative sentences as 0\n",
        "polarities.extend([1] * len(df_sent_pos))  # Label positive sentences as 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f7c9d3-5084-4c58-b1fd-b3efb212313f",
      "metadata": {
        "id": "24f7c9d3-5084-4c58-b1fd-b3efb212313f"
      },
      "outputs": [],
      "source": [
        "# Combine sentences and labels into a single list\n",
        "combined = list(zip(sentences, polarities))\n",
        "\n",
        "# Shuffle the combined list\n",
        "random.shuffle(combined)\n",
        "\n",
        "# Split the shuffled list back into sentences and labels\n",
        "sentences[:], polarities[:] = zip(*combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff035d59-dfb8-4e64-a94c-fbad9d63eafa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff035d59-dfb8-4e64-a94c-fbad9d63eafa",
        "outputId": "f0986b16-86f3-4cd1-aef9-4fd414848974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of training set: 8529\n",
            "Size of test set: 2133\n"
          ]
        }
      ],
      "source": [
        "# Define train-test split ratio\n",
        "train_test_ratio = 0.8\n",
        "\n",
        "# Calculate the size of the training set\n",
        "train_set_size = int(train_test_ratio * len(sentences))\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test = sentences[:train_set_size], sentences[train_set_size:]\n",
        "y_train, y_test = polarities[:train_set_size], polarities[train_set_size:]\n",
        "\n",
        "# Print sizes of training and test sets\n",
        "print(\"Size of training set:\", len(X_train))\n",
        "print(\"Size of test set:\", len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d1e90b-ae28-44cd-9c54-8d6e7b91e97c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83d1e90b-ae28-44cd-9c54-8d6e7b91e97c",
        "outputId": "6888773c-b892-4544-87a9-1028826a1328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Samples: 8529, #Features: 8529\n"
          ]
        }
      ],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "num_samples, num_features = X_train_tfidf.shape\n",
        "print(\"#Samples: {}, #Features: {}\".format(num_samples, num_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d52db49e-3327-41fb-9c6c-177e5a853562",
      "metadata": {
        "id": "d52db49e-3327-41fb-9c6c-177e5a853562"
      },
      "outputs": [],
      "source": [
        "# Import the Logistic Regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train the Logistic Regression classifier\n",
        "logistic_regression_classifier = LogisticRegression(solver=\"sag\").fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c9a2c9-aa92-42cd-9da1-6bac6e68ece7",
      "metadata": {
        "id": "b1c9a2c9-aa92-42cd-9da1-6bac6e68ece7"
      },
      "outputs": [],
      "source": [
        "# Transform the test data\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1dab2d3-c623-434d-bca3-497bda721f69",
      "metadata": {
        "id": "f1dab2d3-c623-434d-bca3-497bda721f69"
      },
      "outputs": [],
      "source": [
        "# Predict polarities for the test data\n",
        "y_pred = logistic_regression_classifier.predict(X_test_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "affe1fad-3adb-4aa8-bdfb-bd343ca45cc3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "affe1fad-3adb-4aa8-bdfb-bd343ca45cc3",
        "outputId": "741a9aad-a239-4402-bc40-c1f3b110d704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.76      0.75      1065\n",
            "           1       0.75      0.75      0.75      1068\n",
            "\n",
            "    accuracy                           0.75      2133\n",
            "   macro avg       0.75      0.75      0.75      2133\n",
            "weighted avg       0.75      0.75      0.75      2133\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import evaluation metrics\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Generate the classification report\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de0e212b-9311-49d7-adee-d90fe690b747",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de0e212b-9311-49d7-adee-d90fe690b747",
        "outputId": "e42d9235-cddd-43fe-a2b0-fa1635dda2af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Scores for each fold: [0.75917431 0.72596154 0.77828054 0.74047619 0.7738815  0.74197384\n",
            " 0.76830732 0.71856287 0.76798144 0.73786408]\n",
            "F1 Score (Mean/Average): 0.751\n",
            "F1 Score (Standard Deviation): 0.020\n"
          ]
        }
      ],
      "source": [
        "# Import necessary library\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Perform 10-fold cross-validation on the training data\n",
        "f1_scores_list = cross_val_score(\n",
        "    LogisticRegression(),            # Model: Logistic Regression\n",
        "    X_train_tfidf,                   # Features: TF-IDF transformed training data\n",
        "    y_train,                         # Labels: Training labels\n",
        "    cv=10,                           # Number of folds\n",
        "    scoring=\"f1\"                     # Evaluation metric: F1 score\n",
        ")\n",
        "\n",
        "# Display the F1 scores for each fold\n",
        "print(f\"F1 Scores for each fold: {f1_scores_list}\")\n",
        "\n",
        "# Calculate and display the mean and standard deviation of the F1 scores\n",
        "print(\"F1 Score (Mean/Average): {:.3f}\".format(f1_scores_list.mean()))\n",
        "print(\"F1 Score (Standard Deviation): {:.3f}\".format(f1_scores_list.std()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c183ba-0f53-46ab-b856-18aa45319861",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76c183ba-0f53-46ab-b856-18aa45319861",
        "outputId": "11cd6c06-fdfd-4c8b-b46f-5fde5cf81794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier: LinearSVC, n-gram size: 1 => F1-score: 0.749\n",
            "Classifier: LinearSVC, n-gram size: 2 => F1-score: 0.762\n",
            "Classifier: LinearSVC, n-gram size: 3 => F1-score: 0.759\n",
            "Classifier: LinearSVC, n-gram size: 4 => F1-score: 0.754\n",
            "Classifier: LogisticRegression, n-gram size: 1 => F1-score: 0.751\n",
            "Classifier: LogisticRegression, n-gram size: 2 => F1-score: 0.747\n",
            "Classifier: LogisticRegression, n-gram size: 3 => F1-score: 0.744\n",
            "Classifier: LogisticRegression, n-gram size: 4 => F1-score: 0.741\n",
            "\n",
            "Best Configuration:\n",
            "Classifier: LinearSVC, Max n-gram size: 2, F1-score: 0.762\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Initialize placeholders to store the best configuration\n",
        "best_score = -1.0\n",
        "best_classifier = None\n",
        "best_ngram_size = -1\n",
        "\n",
        "# Define the hyperparameters to test\n",
        "classifiers = [LinearSVC(), LogisticRegression(solver=\"sag\")]\n",
        "ngram_sizes = [1, 2, 3, 4]\n",
        "\n",
        "# Loop through all combinations of classifiers and n-gram sizes\n",
        "for classifier in classifiers:\n",
        "    for n in ngram_sizes:\n",
        "        # Define the vectorizer with the current n-gram size\n",
        "        vectorizer = TfidfVectorizer(ngram_range=(1, n))\n",
        "        X_train_tfidf = vectorizer.fit_transform(X_train)  # Transform training data\n",
        "\n",
        "        # Perform 10-fold cross-validation\n",
        "        f1_scores = cross_val_score(classifier, X_train_tfidf, y_train, cv=10, scoring='f1')\n",
        "        avg_f1_score = f1_scores.mean()  # Calculate average F1-score\n",
        "\n",
        "        # Print the result for this combination\n",
        "        print(f\"Classifier: {type(classifier).__name__}, n-gram size: {n} => F1-score: {avg_f1_score:.3f}\")\n",
        "\n",
        "        # Save the best configuration\n",
        "        if avg_f1_score > best_score:\n",
        "            best_score = avg_f1_score\n",
        "            best_classifier = classifier\n",
        "            best_ngram_size = n\n",
        "\n",
        "# Print the best configuration\n",
        "print(\"\\nBest Configuration:\")\n",
        "print(f\"Classifier: {type(best_classifier).__name__}, Max n-gram size: {best_ngram_size}, F1-score: {best_score:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03370708-7017-41de-9b10-53868357cee6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03370708-7017-41de-9b10-53868357cee6",
        "outputId": "5388dce9-71aa-4116-cf51-e5cba036c7d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Model Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.75      0.76      1065\n",
            "           1       0.75      0.77      0.76      1068\n",
            "\n",
            "    accuracy                           0.76      2133\n",
            "   macro avg       0.76      0.76      0.76      2133\n",
            "weighted avg       0.76      0.76      0.76      2133\n",
            "\n",
            "Accuracy: 0.760\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Use the best configuration to train the final model\n",
        "final_vectorizer = TfidfVectorizer(ngram_range=(1, best_ngram_size))\n",
        "X_train_tfidf = final_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = final_vectorizer.transform(X_test)\n",
        "\n",
        "best_classifier.fit(X_train_tfidf, y_train)\n",
        "y_pred = best_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate and display results\n",
        "print(\"\\nFinal Model Results:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac39d4d-87a6-4fa8-adf5-b8ac4c170323",
      "metadata": {
        "id": "6ac39d4d-87a6-4fa8-adf5-b8ac4c170323"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}